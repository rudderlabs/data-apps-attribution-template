{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf3f5fc-3923-4546-937e-c3394bb0dfbf",
   "metadata": {},
   "source": [
    "## Attribution\n",
    "\n",
    "Attribution is a problem of answering questions related to how different campaigns are driving conversions. These campaigns can be marketing campaigns, newsletters, notifications etc - essentially any sort of _event_ that _touches_ a user. Hence these events are called _touches_. \n",
    "\n",
    "Users often do not convert on the very first time they see an ad or an email campaign. It takes multiple exposures before they finally convert. Hence, not surprisingly, there are different ways to answer this simple question. Some of the common methods to measure attribution are last touch and first touch attribution, where a conversion is attributed to either the very last or the very first touch of a user. But there are other methods too, where multiple touches -those that helped a conversion eventually- get partial credits from a single conversion. More details can be found from this [blog](https://www.rudderstack.com/blog/from-first-touch-to-multi-touch-attribution-with-rudderstack-dbt-and-sagemaker/).\n",
    "\n",
    "Below, we see the whole process of calculating the attribution values using following approaches:\n",
    "1. Multi-touch attribution methods:\n",
    "    * Shapley values\n",
    "    * Markov Chain Analysis\n",
    "2. Single touch methods:\n",
    "    * Firt touch\n",
    "    * Last touch\n",
    "    \n",
    "* Shapley values code is implemented based on the logic presented in this [paper](https://arxiv.org/pdf/1804.05327.pdf)\n",
    "* Markov chain values are based on the following [whitepaper](https://www.channelattribution.net/pdf/Whitepaper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required libraries\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gzip\n",
    "from typing import List, Optional, Union, Dict, Tuple\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import yaml\n",
    "import itertools\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a540c0e",
   "metadata": {},
   "source": [
    "### Part I: CONFIG:\n",
    "\n",
    "Loading configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b8857",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell for papermill. These values can get overridden by parameters passed by papermill\n",
    "run_id = str(int(time.time()))\n",
    "folder_utils_path = None # \"/opt/ml/processing/input/code/\")\n",
    "local_output_path = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df581703",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_id:\n",
    "    run_id = str(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3615fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if folder_utils_path:\n",
    "    sys.path.append(folder_utils_path)\n",
    "    \n",
    "from utils import create_logger\n",
    "from load_data import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FORMAT = 'png'\n",
    "try:\n",
    "    logging = create_logger(os.path.join(\"logs\",\"multi_touch_attribution.log\"))\n",
    "except Exception as e:\n",
    "    #print(str(e))\n",
    "    pass\n",
    "\n",
    "logging.info(\"\\n\\n\\t\\tSTARTING FEATURE PREPROCESSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159acbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if folder_utils_path is None:\n",
    "    logging.info(\"Running locally\")\n",
    "    mode = \"local\"\n",
    "    config_path = \"config/analysis_config.yaml\"\n",
    "else:\n",
    "    logging.info(\"Running inside a container\")\n",
    "    mode = \"container\"\n",
    "    config_path = \"/opt/ml/processing/code/config/analysis_config.yaml\"\n",
    "    \n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Config used:\")\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mode used: {mode}\")\n",
    "with open(config[\"mode\"][mode][\"wh_credentials_path\"], \"r\") as f:\n",
    "    creds = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64b84d",
   "metadata": {},
   "source": [
    "### Part II: Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3335e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = creds[\"data_warehouse\"].get(\"database\")\n",
    "schema = creds[\"data_warehouse\"].get(\"schema\")\n",
    "table = creds[\"data_warehouse\"].get(\"feature_registry_table\")\n",
    "results_table = creds[\"data_warehouse\"].get(\"prediction_output_table_name\")\n",
    "\n",
    "#Read configurations for data preparation\n",
    "min_date = config[\"data\"][\"min_date\"]\n",
    "ignore_events = config[\"data\"][\"ignore_events\"]\n",
    "\n",
    "primary_key_column = config[\"data\"][\"primary_key_column\"]\n",
    "events_column_name = config[\"data\"][\"events_column_name\"]\n",
    "timestamp_column_name = config[\"data\"][\"timestamp_column_name\"]\n",
    "\n",
    "# Once data is loaded, these are used in the notebook to do data transformations and cleanup\n",
    "conversion_event_name = config[\"data\"][\"conversion_event_name\"]\n",
    "group_events = config[\"data\"][\"group_events\"]\n",
    "group_events_mapping = config[\"data\"][\"group_events_mapping\"]\n",
    "filter_columns = config[\"data\"][\"filter_columns\"]\n",
    "min_event_interval_in_sec = config[\"analysis\"][\"min_event_interval_in_sec\"]\n",
    "\n",
    "if group_events_mapping:\n",
    "    events_type_mapping = reduce(lambda x, y: {**x,**y}, [{val:key for val in list_vals} for key, list_vals in group_events_mapping.items()])\n",
    "else:\n",
    "    events_type_mapping = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cded775-72e8-4c64-86e5-150c7ca89d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the output files get stored in the output_directory. Each run of the feature_processing generates a new sub directory based on the timestamp.\n",
    "# output directory structure\n",
    "# - data\n",
    "#   - <run_id>\n",
    "#      \n",
    "\n",
    "output_directory = os.path.join(local_output_path, run_id)\n",
    "\n",
    "logging.info(f\"All the output files will be saved to following location: {output_directory}\")\n",
    "Path(output_directory).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc9cf9-7597-4e83-bed8-6cf2f65d327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"WH multi touch data config: database: {database}, schema: {schema}, table: {table}\")\n",
    "table_name =  f\"{database}.{schema}.{table}\"\n",
    "print(f\"Following table from warehouse is being used to read the user touches: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be85f56-14d9-4e13-9b73-cf018f6f50df",
   "metadata": {},
   "source": [
    "## Getting data from the warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e9da6-b32d-4bfc-991b-335be5e2db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_query(entity_key_col: str,\n",
    "                  event_col: str,\n",
    "                  ts_col: str,\n",
    "                  table_name: str, \n",
    "                  ignore_events_list: Optional[List[str]]=None,\n",
    "                  start_date: Optional[str]=None, \n",
    "                  extra_cols_list: Optional[List[str]]=None) -> str:\n",
    "    all_columns = [entity_key_col, event_col, ts_col]\n",
    "    if extra_cols_list is not None:\n",
    "        all_columns = all_columns + extra_cols_list\n",
    "    all_columns_str = ', '.join(all_columns)\n",
    "    query = f\"select {all_columns_str} from {table_name}\"\n",
    "    conditions = []\n",
    "    if ignore_events_list is not None and len(ignore_events_list) > 0:\n",
    "        ignore_events_substr = \", \".join([f\"'{e}'\" for e in ignore_events_list])\n",
    "        ignore_events_cond = f\"{event_col} not in ({ignore_events_substr})\"\n",
    "        conditions.append(ignore_events_cond)\n",
    "\n",
    "    if start_date is not None:\n",
    "        min_date_cond = f\"{ts_col} >= '{start_date}'\"\n",
    "        conditions.append(min_date_cond)\n",
    "        \n",
    "    conditions_str = ' and '.join(conditions)\n",
    "    if conditions_str:\n",
    "        return f\"{query} where {conditions_str}\"\n",
    "    else:\n",
    "        return query\n",
    "    \n",
    "\n",
    "# Test cases:\n",
    "assert prepare_query('user_id', 'event_name','ts', 'table') == 'select user_id, event_name, ts from table'\n",
    "assert prepare_query('user_id', 'event_name','ts', 'table', ['v1','v2']) == \"select user_id, event_name, ts from table where event_name not in ('v1', 'v2')\"\n",
    "assert prepare_query('user_id', 'event_name','ts', 'table', \n",
    "                     ['v1','v2'], '2022-02-02') == \"select user_id, event_name, ts from table where event_name not in ('v1', 'v2') and ts >= '2022-02-02'\"\n",
    "\n",
    "assert prepare_query('user_id', 'event_name','ts', 'table', None, '2022-02-02') == \"select user_id, event_name, ts from table where ts >= '2022-02-02'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91b2c8-af9f-427f-bff2-b6a3706d4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = prepare_query(primary_key_column, events_column_name, timestamp_column_name, table_name, ignore_events, min_date)\n",
    "print(f\"Following query reads all the necessary data from the warehouse:\\n\\t{query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a870a-9e1b-4915-b99b-6037c052b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_conn = Connector(creds[\"data_warehouse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35405f-8d42-42b2-bc8a-001ef352f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "raw_data = wh_conn.run_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5dab79-e210-4fc5-9666-219bc3b90ec1",
   "metadata": {},
   "source": [
    "Sample data of the raw data from warehouse, fetched using the above query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fa207-aeb2-41da-81a0-280ec7ce7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17138e-7a44-4b02-954f-6c727b47702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"No:of data points in the raw table: {len(raw_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26392da3-c575-4a68-8dec-003ee7416967",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In this table, all rows with the value of '{conversion_event_name}' in the column {events_column_name} are considered as conversion events. Rest all are considered as touches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836eb205-719c-43a5-a671-b152f9aede2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_timestamps = raw_data.query(f\"{events_column_name}=='{conversion_event_name}'\").groupby(primary_key_column)[timestamp_column_name].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b823b9-c13a-4c22-ae9e-731836290b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_ts_col = f\"converted_{timestamp_column_name}\"\n",
    "\n",
    "event_data = (raw_data\n",
    "              .query(f\"{events_column_name}!='{conversion_event_name}'\")\n",
    "              .merge(conversion_timestamps, \n",
    "                     how=\"left\", \n",
    "                     left_on=primary_key_column, \n",
    "                     right_index=True)\n",
    "              .rename(columns={f\"{timestamp_column_name}_x\": timestamp_column_name, f\"{timestamp_column_name}_y\": converted_ts_col})\n",
    "              .query(f\"{converted_ts_col}.isnull() or {timestamp_column_name}<={converted_ts_col}\", engine=\"python\")\n",
    "              .drop_duplicates()\n",
    "             )\n",
    "\n",
    "print(f\"No:of data points after some basic clean up such as de-duplicating, and separating out conversion events: {len(event_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc636e5b-5fb3-4b77-a819-55c620c77624",
   "metadata": {},
   "source": [
    "Following are the touches related data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884168ac-c6b9-40b2-804d-65700df1ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Distinct touches count: {len(event_data[events_column_name].value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d2e22-5561-4150-acde-64b023cd9431",
   "metadata": {},
   "source": [
    "Top 20 touches by percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1865bd-e2a4-4f16-aa83-76a2799a9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(event_data[events_column_name].value_counts(normalize=True).head(20).round(4)*100).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369096d8-fd19-4bb3-9149-8b215ec30a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_touches(df: pd.DataFrame, event_col: str, top_k: Optional[int]=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Picks only the top k touches and groups rest all as 'others'. If 'others' is one of the events, it appends that with current epoch time\n",
    "    \"\"\"\n",
    "    if top_k is None:\n",
    "        return df\n",
    "    else:\n",
    "        top_k_events = list(df[event_col].value_counts().head(top_k).index)\n",
    "        all_events = list(df[event_col].value_counts().index)\n",
    "        default_event = 'others'\n",
    "        if default_event in all_events:\n",
    "            curr_time = int(time.time())\n",
    "            while f\"{default_event}_{curr_time}\" in all_events:\n",
    "                curr_time+=1\n",
    "            default_event = f\"{default_event}_{curr_time}\"\n",
    "        df[event_col] =  df[event_col].apply(lambda event: event if event in top_k_events else default_event)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Test cases:\n",
    "test_df = pd.DataFrame.from_dict({\"uid\":[1,2,3],\"event\":['e1','e1','e2']})\n",
    "assert (get_top_k_touches(test_df, 'event') == test_df ).all().all()\n",
    "assert (get_top_k_touches(test_df, 'event',1) ==  pd.DataFrame.from_dict({\"uid\":[1,2,3],\"event\":['e1','e1','others']})).all().all()\n",
    "\n",
    "test_df = pd.DataFrame.from_dict({\"uid\":[1,2,3],\"event\":['e1','e1','others']})\n",
    "\n",
    "assert (get_top_k_touches(test_df, 'event',1) ==  pd.DataFrame.from_dict({\"uid\":[1,2,3],\"event\":['e1','e1',f'others_{int(time.time())}']})).all().all()\n",
    "\n",
    "curr_time = int(time.time())\n",
    "test_df = pd.DataFrame.from_dict({\"uid\":[1,2,3,4],\"event\":['e1','e1','others',f'others_{curr_time}']})\n",
    "\n",
    "try:\n",
    "    assert (get_top_k_touches(test_df, 'event',1) ==  pd.DataFrame.from_dict({\"uid\":[1,2,3,4],\"event\":['e1','e1',f'others_{curr_time+1}',f'others_{curr_time+1}']})).all().all()\n",
    "except AssertionError:\n",
    "    print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79c946-3512-44e5-bb6f-aa7ac4f54b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if config[\"data\"][\"n_top_events\"] is not None:\n",
    "    n_top_k_events = config[\"data\"][\"n_top_events\"]\n",
    "    print(\"Having too many touches would make calculating Shapley values challenging, and also make it difficult to interpret the results.\")\n",
    "    print(f\"So, as a default option, only the top {n_top_k_events} events by vol are considered. Rest are all grouped as one single touch type. This behavior can be modified from the config file.\")\n",
    "    event_data = get_top_k_touches(event_data, events_column_name, n_top_k_events)\n",
    "    print(f\"Percent touches replaced by default value: {event_data[events_column_name].value_counts(normalize=True)['others'] * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations on the raw data. We apply the constraints defined in the constants cell above.\n",
    "\n",
    "def dedup_by_ts_delta(df: pd.DataFrame, primary_key: str, timestamp: str, event_type: str, max_lag: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ### Parameters\n",
    "    1. df : pd.DataFrame\n",
    "        - User touches dataframe. \n",
    "    2. primary_key : str\n",
    "        - column name of the column that contains user_id. \n",
    "    3. timestamp: str\n",
    "        - column name of the column that contains event timestamp\n",
    "    4. event_type: str\n",
    "        - column name of the column that contains event/touch data\n",
    "    5. max_lag: int\n",
    "        - max time (in sec) between consecutive events to be considered as duplicates. \n",
    "\n",
    "    ### Returns\n",
    "    - DataFrame after doing following steps\n",
    "    Based on primary key and event_type, it checks if two consecutive events occur within the max_lag time window. If so, they are considered same event and the latter event is dropped. \n",
    "    \"\"\"\n",
    "    if max_lag <= 0:\n",
    "        return df\n",
    "    df = df.sort_values(by=[primary_key, timestamp], ascending=True).reset_index(drop=True)\n",
    "    original_columns = df.columns\n",
    "    df[f\"prev_{primary_key}\"] = df[primary_key].shift()\n",
    "    df[f\"prev_{event_type}\"] = df[event_type].shift()\n",
    "    df[f\"prev_{timestamp}\"] = df[timestamp].shift()\n",
    "\n",
    "    def is_duplicate(row):\n",
    "        if pd.isnull(row[f\"prev_{primary_key}\"]) or pd.isnull(row[f\"prev_{event_type}\"]) or pd.isnull(row[f\"prev_{timestamp}\"]):\n",
    "            return False\n",
    "        elif row[primary_key] == row[f\"prev_{primary_key}\"] and row[event_type] == row[f\"prev_{event_type}\"] and (\n",
    "                row[timestamp] - row[f\"prev_{timestamp}\"]).total_seconds() <= max_lag:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    df[\"is_duplicate\"] = df.progress_apply(is_duplicate, axis=1)\n",
    "    return df.query(\"is_duplicate==False\")[original_columns].reset_index(drop=True)\n",
    "\n",
    "def process_raw_data(raw_data_df: pd.DataFrame,\n",
    "                     dedup_min_time: int,\n",
    "                     reduce_touches: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ### Parameters\n",
    "    1. raw_data_df : Raw data \n",
    "    2. ignore_touches: Ignores the touches present in this list. \n",
    "    3. min_date: Ignores events before this date\n",
    "    4. dedup_min_time: Time (in sec) between two events of same type. Events that repeat within this interval are combined as one (earlier timestamp is considered)\n",
    "    5. reduce_touches : Whether to combine touchpoints based on their logical groupings\n",
    "\n",
    "    ### Returns\n",
    "    - DataFrame after doing following steps\n",
    "    1. Groups tracks pages if reduce_touches flag is True\n",
    "    2. Deduplicates based on 5 min interval \n",
    "    3. Ignores touches based on ignore_touches list\n",
    "    4. Ãrops events before the min_date timestamp.\n",
    "    \"\"\"\n",
    "    if reduce_touches:\n",
    "        raw_data_df[events_column_name] = raw_data_df[events_column_name].apply(lambda touch: events_type_mapping.get(touch, touch))\n",
    "    \n",
    "    dedup_data_df = (dedup_by_ts_delta(raw_data_df\n",
    "                                       .query(f\"~{events_column_name}.isnull()\",engine='python')\n",
    "                                       .drop_duplicates(),\n",
    "                                       primary_key_column,\n",
    "                                       timestamp_column_name, \n",
    "                                       events_column_name, \n",
    "                                       dedup_min_time)\n",
    "                     .filter(filter_columns)\n",
    "                    )\n",
    "    return dedup_data_df.query(f'{events_column_name} not in @ignore_events', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7bdc9c-f745-4a17-92f3-a0978adcf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "touch_data_filtered = process_raw_data(event_data, min_event_interval_in_sec, group_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04d647-0c98-4405-87ad-b1d1feef6b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "touch_data_filtered[converted_ts_col] = touch_data_filtered[primary_key_column].apply(lambda entity: conversion_timestamps.get(entity))\n",
    "touch_data_filtered[\"is_converted\"] = touch_data_filtered[primary_key_column].apply(lambda entity: 1 if entity in conversion_timestamps else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data:\n",
    "touch_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df34278",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_touchpoints = touch_data_filtered.query(\"is_converted==1\", engine='python')\n",
    "negative_touchpoints = touch_data_filtered.query(\"is_converted==0\", engine='python')\n",
    "\n",
    "print(\"Summary stats on converted and non converted journeys:\\n\")\n",
    "print(f\"Total rows (events/touches) in converted journeys: {len(positive_touchpoints)}\")\n",
    "print(f\"Distinct converted journeys: {len(positive_touchpoints[primary_key_column].unique())}\\n\")\n",
    "print(f\"Total rows (events/touches) in non-converted journeys: {len(negative_touchpoints)}\")\n",
    "print(f\"Distinct non-converted journeys: {len(negative_touchpoints[primary_key_column].unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a0610",
   "metadata": {},
   "source": [
    "### Part III: Data distribution (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e7465",
   "metadata": {},
   "source": [
    "A few stats on the converted journeys before going into the actual attribution problem:\n",
    "\n",
    "Note: This section is not required to calculate the attribution values. Instead, it is just to show the distribution of no:of events and no:of days before users convert. You can skip directly to [Part IV](#MTA-Calculations-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_summary = positive_touchpoints.groupby([primary_key_column]).agg({timestamp_column_name: \"min\", events_column_name:[\"size\", \"nunique\"]}).reset_index()\n",
    "col_n_events = f\"n_{events_column_name}\"\n",
    "col_n_distinct_events = f\"n_distinct_{events_column_name}\"\n",
    "\n",
    "conversion_summary.columns = [primary_key_column, timestamp_column_name, col_n_events, col_n_distinct_events]\n",
    "conversion_summary[\"days_to_convert\"] = conversion_summary.apply(lambda row: (conversion_timestamps.get(row[primary_key_column]) - row[timestamp_column_name]).days, axis=1)\n",
    "\n",
    "print(\"Sample user level summary of no:of events and no:of days to convert:\")\n",
    "conversion_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8347c76-1a42-4ae1-befe-6b6c24cb6baf",
   "metadata": {},
   "source": [
    "In the below images, the distribution of conversions is shown - first based on the no:of total events before conversion, and the second one based on no:of days since first seen.\n",
    "\n",
    "The x-axis of days since first seen plot is capped at 100 days by default. It can be modifid by changing the variable `MAX_CONVERSION_DAYS` in the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONVERSION_DAYS=100 # This is used only for visualizing below, and does not have any other affect.\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(16,6))\n",
    "sns.histplot(conversion_summary[col_n_events], bins=50, ax=axs[0])\n",
    "axs[0].set_title(\"No:of events before conversion\")\n",
    "axs[0].set_ylabel(\"Conversions\")\n",
    "axs[0].set_xlabel(\"Event count\");\n",
    "\n",
    "sns.histplot(conversion_summary[\"days_to_convert\"], bins=70, ax=axs[1])\n",
    "axs[1].set_title(\"Days to conversion\")\n",
    "axs[1].set_ylabel(\"Conversions\")\n",
    "axs[1].set_xlim([0,MAX_CONVERSION_DAYS])\n",
    "axs[1].set_xlabel(\"Days since first seen\");\n",
    "\n",
    "plt.savefig(os.path.join(output_directory, f\"data_distribution.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978dcfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks percentile counts at various levels. Can modify this list to get a different percentile value (ex: For 90th percentile, add 90 to the list.)\n",
    "percentile_points = [0,5, 25, 50, 75, 95, 99, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0da494",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Avg no:of touches before a user converts: \\n\\nMean: {conversion_summary[col_n_events].mean():.2f}\\nMedian: {np.median(conversion_summary[col_n_events])}\")\n",
    "print(\"\\nPercentiles for No:of touches:\")\n",
    "(pd.DataFrame.from_dict(\n",
    "    dict(zip(percentile_points, \n",
    "             np.percentile(conversion_summary[col_n_events],\n",
    "                           percentile_points).round())),\n",
    "    orient='index', \n",
    "    columns=[col_n_events])\n",
    " .reset_index().rename(columns={\"index\":\"percentile\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ee237-4b9c-4d68-8f30-3650aa1ac024",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Avg days to convert:\\n\\nMean:{conversion_summary['days_to_convert'].mean():.2f}\\nMedian: {np.median(conversion_summary['days_to_convert'])}\")\n",
    "print(\"\\nPercentiles for No:of days to convert:\")\n",
    "(pd.DataFrame.from_dict(\n",
    "    dict(zip(percentile_points, \n",
    "             np.percentile(conversion_summary['days_to_convert'],\n",
    "                           percentile_points).round())),\n",
    "    orient='index', \n",
    "    columns=['n_days_to_convert'])\n",
    " .reset_index().rename(columns={\"index\": \"percentile\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7c7c31-6a66-41f1-a0a9-e55d4d4539eb",
   "metadata": {},
   "source": [
    "### Part IV: Data transformation:\n",
    "\n",
    "Converting data to the required format for calculating attribution values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6769b-35ec-41b8-a226-694094da3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function to transform data to the required input form \n",
    "\n",
    "def collect_touchpoints(touchpoints_df: pd.DataFrame, \n",
    "                        primary_key: str=primary_key_column, \n",
    "                        ts_column: str=timestamp_column_name,\n",
    "                        touchpoint_column: str=events_column_name) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform dataframe with each touch as a row to a new dataframe where each journey has a single row with all touches as a list in chronological order\n",
    "    touchpoints_df: A dataframe with each row corresponding to a touch. \n",
    "    primary_key: Name of the column containing unique user identifier\n",
    "    ts_column: Name of column containing timestamp using which journeys are sorted chronologically\n",
    "    touchpoint_column: Name of column containing touch points. \n",
    "    \n",
    "    Returns:\n",
    "    A dataframe with two columns, first has primary key and second has the touchpoint_column\n",
    "    \"\"\"\n",
    "    return (touchpoints_df\n",
    "            .sort_values(by=[primary_key, ts_column], \n",
    "                         ascending=True)\n",
    "            .reset_index(drop=True)\n",
    "            .groupby(primary_key)[touchpoint_column]\n",
    "            .apply(list)\n",
    "            .reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "touchpoints_list_pos = collect_touchpoints(positive_touchpoints)\n",
    "touchpoints_list_neg = collect_touchpoints(negative_touchpoints)\n",
    "\n",
    "touchpoints_list_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff43a7c-e258-4144-ae19-7fed6936581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(touchpoints_list_pos), len(touchpoints_list_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349efe4",
   "metadata": {},
   "source": [
    "<a id='MTA-Calculations-begin'></a>\n",
    "\n",
    "### Part V: Shapley values calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "touches_shapley_values = get_shapley_values(touchpoints_list_pos[events_column_name].values, [1] * len(touchpoints_list_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df818e",
   "metadata": {},
   "source": [
    "### Part VI: Markov Chain Values Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b2593",
   "metadata": {},
   "source": [
    "Index of transition counts: 1st: source, last: destination. 2 to -1: same order as dict_touches_inv keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d613e6f9-572b-4540-9d89-bb4b1cbdbe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_touches = list(touch_data_filtered[events_column_name].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b083a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_markov = False\n",
    "try:\n",
    "    markov_attribution_values, transition_probabilities = get_markov_attribution(touchpoints_list_pos[events_column_name].values, \n",
    "                                                                                 touchpoints_list_neg[events_column_name].values, \n",
    "                                                                                 all_touches,\n",
    "                                                                                 visualize=True)\n",
    "\n",
    "    plt.savefig(os.path.join(output_directory, f\"markov_transition_probabilities.{IMAGE_FORMAT}\"))\n",
    "    flag_markov = True\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    flag_markov = False\n",
    "    markov_attribution_values = None\n",
    "    transition_probabilities = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc6a57",
   "metadata": {},
   "source": [
    "In the above graphic, we can see the transition probabilities from each touch (Y-axis) to the next touch (X-axis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b369296",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pos_transitions, labels = generate_transition_counts(touchpoints_list_pos[events_column_name].values, all_touches, is_positive=True)\n",
    "    neg_transitions, labels = generate_transition_counts(touchpoints_list_neg[events_column_name].values, all_touches, is_positive=False)\n",
    "    all_transitions = pos_transitions + neg_transitions\n",
    "\n",
    "    fig, axs=plt.subplots(1,2, figsize=(18, 5))\n",
    "    sns.set_style(\"white\")\n",
    "    sns.barplot(x=labels[:-2], y=100*all_transitions[:-2, -2]/all_transitions[:-2, -2].sum(), ci=None, color=\"salmon\", ax=axs[0])\n",
    "\n",
    "    axs[0].set_xticklabels(labels[:-2],rotation=60)\n",
    "    axs[0].set_ylabel(\"Percent\")\n",
    "    axs[0].set_title(\"Distribution of last touch before dropoff\");\n",
    "\n",
    "    sns.barplot(x=labels[:-2], y=100*all_transitions[:-2, -1]/all_transitions[:-2, -1].sum(), ci=None, color=\"salmon\", ax=axs[1])\n",
    "    axs[1].set_xticklabels(labels[:-2],rotation=60)\n",
    "    axs[1].set_ylabel(\"Percent\")\n",
    "    axs[1].set_title(\"Distribution of last touch before convert\");\n",
    "\n",
    "    plt.savefig(os.path.join(output_directory, f\"distribution_of_last_touch_before_dropoff_and_convert.{IMAGE_FORMAT}\"))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485eabe7-ac0c-449d-a5b9-7e32838317c1",
   "metadata": {},
   "source": [
    "\n",
    "The left plot above tells the distribution of all the touches immediately before they drop off. All the bars sum up to 100.\n",
    "\n",
    "The right plot above tells the distribution of all the touches immediately before they convert. All the bars sum up to 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee27fd6",
   "metadata": {},
   "source": [
    "### Part VII: Conclusion - Results summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a7ace-a167-4add-ad14-4a0cc883d9e7",
   "metadata": {},
   "source": [
    "The attribution values from different methods look as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03f537-face-4f65-a2c9-009a0a02486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_touch_results = get_single_touch_attribution(touchpoints_list_pos, events_column_name, last_touch=True, normalize=False)\n",
    "first_touch_results = get_single_touch_attribution(touchpoints_list_pos, events_column_name, last_touch=False, normalize=False)\n",
    "\n",
    "mta_values = merge_dictionaries([touches_shapley_values, markov_attribution_values, last_touch_results, first_touch_results] , ['shap', 'markov', 'last_touch', 'first_touch'])\n",
    "\n",
    "mta_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40484ef-93ee-49ef-9a68-f2e34c1e0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In the table, each column shows what the attribution scores for different touches are.\\n\")\n",
    "\n",
    "print(f\"For example, the first value in the column corresponding to `last_touch` tell how many conversions can be attributed to the touch type '{mta_values.index[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124e79c-37b6-4670-927d-ebe97fccea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Each column independently sums to the total number of conversions, as seen below: \\n\")\n",
    "mta_values.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f80983-2dab-4dfc-941a-c9d494201c04",
   "metadata": {},
   "source": [
    "The same data is shown as a visualization below. In it, x-axis has different touches and y-axis has the no:of conversions attributable to each touch. The color of the bar shows what method is used to capture the attribution value. A high attribution score indicates high conversions coming from that touch point. A low score indicates low conversions coming from that touch point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_long = pd.melt(mta_values.reset_index(), 'index', list(mta_values))\n",
    "mta_long.columns = ['touch', 'method', 'attribution']\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.barplot(data=mta_long, x='touch',y='attribution',hue='method');\n",
    "plt.xticks(rotation=90);\n",
    "plt.savefig(os.path.join(output_directory, f\"results_summary.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b9025-c2cb-4d33-ba7b-35174b5a4106",
   "metadata": {},
   "source": [
    "## Appendix:\n",
    "\n",
    "The attribution values are already computed above. Below parts are not necessary but rather sanity checks on the robustness of results and how correlated results from different methods are. Going through them is entirely optional to understand the attribution results themselves, and is needed only as a QA on the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5604fcf2-e21a-48fe-a0f8-eed9aa96b3dc",
   "metadata": {},
   "source": [
    "**Correlation:**\n",
    "\n",
    "Correlation between different methods is shown in the below table. If the score is close to 1, that indicates both methods give very similar results, ex, attribution scores using shapley values and markov chain method are very close to each other.\n",
    "\n",
    "Instead if the score is close to 0, that indicates the two methods give results that are fairly independent of each other. Say, shapley values and markov chain methods have a correlation of 0.05, that indicates the touches deemed important by both methods are very different.\n",
    "\n",
    "Usually if the number of distinct touches before a conversion is low, the correlation would tend to be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_values.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc0c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ToDo: Writing results back to redshift table is pending. \n",
    "# SnowflakeDataIO.write_to_snowflake_table(mta_values, creds[\"snowflake\"][\"results_table_name\"], creds[\"snowflake\"], if_exists=\"append\")\n",
    "# print(f'The output data is stored in the warehouse table: {creds[\"snowflake\"][\"results_table_name\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa833db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MTA values will be written to the location:\\n\\t{output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a140392",
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_values.to_parquet(f\"{output_directory}/mta_values.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2480784c",
   "metadata": {},
   "source": [
    "**Robustness testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30783a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_vals = {}\n",
    "shapley_vals = {}\n",
    "\n",
    "touchpoints_list_pos_rand1, touchpoints_list_pos_rand2 = train_test_split(touchpoints_list_pos, train_size=0.5)\n",
    "touchpoints_list_neg_rand1, touchpoints_list_neg_rand2 = train_test_split(touchpoints_list_neg, train_size=0.5)\n",
    "\n",
    "touches_shapley_values_rand1 = get_shapley_values(touchpoints_list_pos_rand1[events_column_name].values, [1] * len(touchpoints_list_pos_rand1))\n",
    "# get_shapley_values(touchpoints_list_pos_rand1[\"event_id\"].values, dict_touches, visualize=False)\n",
    "markov_attribution_values_rand1, _ = get_markov_attribution(touchpoints_list_pos_rand1[events_column_name].values, touchpoints_list_neg_rand1[events_column_name].values, all_touches, visualize=False)\n",
    "\n",
    "touches_shapley_values_rand2 = get_shapley_values(touchpoints_list_pos_rand2[events_column_name].values, [1] * len(touchpoints_list_pos_rand2))\n",
    "markov_attribution_values_rand2, _ = get_markov_attribution(touchpoints_list_pos_rand2[events_column_name].values, touchpoints_list_neg_rand2[events_column_name].values, all_touches, visualize=False)\n",
    "\n",
    "\n",
    "for key, val in touches_shapley_values_rand1.items():\n",
    "    shapley_vals[key] = [val, touches_shapley_values_rand2.get(key,0)]\n",
    "\n",
    "for key, val in markov_attribution_values_rand1.items():\n",
    "    markov_vals[key] = [val, markov_attribution_values_rand2.get(key,0)]\n",
    "\n",
    "    \n",
    "shapley_vals_df = pd.DataFrame.from_dict(shapley_vals, orient='index')\n",
    "markov_vals_df = pd.DataFrame.from_dict(markov_vals, orient='index')\n",
    "print(f\"Correlation of shapley values between two non-overlapping splits: {shapley_vals_df.corr()[0][1]:.3f}\")\n",
    "print(f\"Correlation of markov values between two non-overlapping splits: {markov_vals_df.corr()[0][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapley_vals_df = pd.melt(shapley_vals_df.reset_index(), id_vars='index')\n",
    "shapley_vals_df.columns = ['touch', 'iter', 'shap']\n",
    "\n",
    "markov_vals_df = pd.melt(markov_vals_df.reset_index(), id_vars='index')\n",
    "markov_vals_df.columns = ['touch', 'iter', 'markov']\n",
    "\n",
    "tp_order = shapley_vals_df.groupby('touch')['shap'].mean().reset_index().sort_values(\"shap\", ascending=False)['touch'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c30a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16,6))\n",
    "sns.barplot(x='touch', y='shap', hue='iter', data=shapley_vals_df, ax=axs[0], order=tp_order)\n",
    "for item in axs[0].get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "axs[0].set_title(\"Shapley vals for touchpoints in non-overlapping splits\");\n",
    "\n",
    "sns.barplot(x='touch', y='markov', hue='iter', data=markov_vals_df, ax=axs[1], order=tp_order)\n",
    "for item in axs[1].get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "axs[1].set_title(\"Markov vals for touchpoints in non-overlapping splits\");\n",
    "\n",
    "plt.savefig(os.path.join(output_directory, f\"shapley_markov_values_non_overlapping_splits.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_vals = {}\n",
    "shapley_vals = {}\n",
    "for iters in range(10):\n",
    "    touchpoints_list_pos_rand, _ = train_test_split(touchpoints_list_pos, train_size=0.7)\n",
    "    touchpoints_list_neg_rand, _ = train_test_split(touchpoints_list_neg, train_size=0.7)\n",
    "    touches_shapley_values_rand = get_shapley_values(touchpoints_list_pos_rand[events_column_name].values, [1] * len(touchpoints_list_pos_rand))\n",
    "    markov_attribution_values_rand, _ = get_markov_attribution(touchpoints_list_pos_rand[events_column_name].values, touchpoints_list_neg_rand[events_column_name].values, all_touches, visualize=False)\n",
    "    for touch, shap in touches_shapley_values_rand.items():\n",
    "        curr = shapley_vals.get(touch, [])\n",
    "        curr.append(shap)\n",
    "        shapley_vals[touch] = curr\n",
    "    for touch, mark in markov_attribution_values_rand.items():\n",
    "        curr = markov_vals.get(touch, [])\n",
    "        curr.append(mark)\n",
    "        markov_vals[touch] = curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef00dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapley_ranks = pd.DataFrame.from_dict(shapley_vals, orient='index').fillna(0).rank(axis=0, ascending=False).astype(int)\n",
    "markov_ranks = pd.DataFrame.from_dict(markov_vals, orient='index').fillna(0).rank(axis=0, ascending=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee61026-b183-44f9-9d33-bc96381d8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_heatmap(df:pd.DataFrame, title:str)->None:\n",
    "    ax = sns.heatmap(df.sort_values(by=0),\n",
    "                     linewidths=0.5,\n",
    "                     robust=True, \n",
    "                     annot_kws={\"size\":10}, \n",
    "                     annot=True,\n",
    "                     fmt=\"d\",\n",
    "                     cmap=\"YlGnBu\",\n",
    "                     cbar=False)\n",
    "    ax.tick_params(labelsize=14)\n",
    "    ax.figure.set_size_inches((10, 6))\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_title(title);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384acba3-ffe1-477f-a445-fdf8c5b9b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_heatmap(shapley_ranks,\"Shapley Value based value rank in different iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_heatmap(markov_ranks,\"Markov Chain based value rank in different iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f51f3f",
   "metadata": {},
   "source": [
    "In the above two heatmaps, each row shows the rank of that respective touch, within each iteration. If the methods are stable, the ranks don't change much. Some variations are expected, especially when the converted journeys are small in number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b7e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ea2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell to hide code while converting to a html page\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "$('div.input').hide();\n",
    "</script>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f019c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb9dda7c9d815e1cdc337d7fe8d5832923daae4c84b0a4e15a32dd1f30943ba5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
