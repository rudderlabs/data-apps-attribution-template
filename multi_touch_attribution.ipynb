{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9beac57",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "In this notebook, we calculate multi-touch attribution values for a given set of channels using following methods:\n",
    "1. Shapley values - considering only positive conversions\n",
    "2. Shapley values - considering both positive and negative conversions\n",
    "3. Markov chain values \n",
    "4. Last touch based \n",
    "\n",
    "* Shapley values code is implemented based on the logic presented in this [paper](https://arxiv.org/pdf/1804.05327.pdf)\n",
    "* Markov chain values are based on the following [whitepaper](https://www.channelattribution.net/pdf/Whitepaper.pdf)\n",
    "\n",
    "Setup:\n",
    "1. This code runs in AWS Sagemaker.\n",
    "2. Input data is to be prepared using Rudderstack Reverse ETL (more details in accompanying blogpost)\n",
    "3. Edit constants in the cell below. Only the first cell requires editing. No other edit is necessary. \n",
    "4. The output results get posted as a parquet file in the given output. The same can also be viewed in the notebook itself, as we go towards the end. \n",
    "5. Throughout the notebook, along with the code, we can also see several descriptions around the data and the results.\n",
    "6. Towards the end, we also have a robustness check to ensure how robust the results are if input data changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required libraries\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gzip\n",
    "from typing import List, Optional, Union, Dict, Tuple\n",
    "import datetime\n",
    "\n",
    "from math import factorial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "import yaml\n",
    "import itertools\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a540c0e",
   "metadata": {},
   "source": [
    "### Part I: CONFIG:\n",
    "\n",
    "Define constants below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b8857",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell for papermill. These values can get overridden by parameters passed by papermill\n",
    "run_id = str(int(time.time()))\n",
    "folder_utils_path = None # \"/opt/ml/processing/input/code/\")\n",
    "local_output_path = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df581703",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_id:\n",
    "    run_id = str(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3615fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if folder_utils_path:\n",
    "    sys.path.append(folder_utils_path)\n",
    "    \n",
    "from utils import create_logger\n",
    "from load_data import pipe as get_raw_data\n",
    "from load_data import SnowflakeDataIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FORMAT = 'png'\n",
    "try:\n",
    "    logging = create_logger(os.path.join(\"logs\",\"multi_touch_attribution.log\"))\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "    pass\n",
    "\n",
    "logging.info(\"\\n\\n\\t\\tSTARTING FEATURE PREPROCESSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159acbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if folder_utils_path is None:\n",
    "    logging.info(\"Running locally\")\n",
    "    mode = \"local\"\n",
    "    config_path = \"config/analysis_config.yaml\"\n",
    "else:\n",
    "    logging.info(\"Running inside a container\")\n",
    "    mode = \"container\"\n",
    "    config_path = \"/opt/ml/processing/code/config/analysis_config.yaml\"\n",
    "    \n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Config used:\")\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mode used: {mode}\")\n",
    "with open(config[\"mode\"][mode][\"wh_credentials_path\"], \"r\") as f:\n",
    "    creds = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352982e3",
   "metadata": {},
   "source": [
    "**No other manual change is expected. Rest of the code should run without any edit.**\n",
    "\n",
    "You can run the whole notebook by going to `Run -> Run all cells`. This creates the parquet file output in prescribed s3 location, and also prints the data here for instant consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64b84d",
   "metadata": {},
   "source": [
    "### Part II: Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3335e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = creds[\"snowflake\"].get(\"database\") #analytics_db\n",
    "schema = creds[\"snowflake\"].get(\"schema\") # dbt_usecases\n",
    "table = creds[\"snowflake\"].get(\"query_data_table\")# \"lead_scoring_mock_features_materalized\"\n",
    "results_table = creds[\"snowflake\"].get(\"results_table_name\")\n",
    "\n",
    "#Read configurations for data preparation\n",
    "min_date = config[\"data\"][\"min_date\"]\n",
    "ignore_events = config[\"data\"][\"ignore_events\"]\n",
    "group_events = config[\"data\"][\"group_events\"]\n",
    "group_events_mapping = config[\"data\"][\"group_events_mapping\"]\n",
    "\n",
    "primary_key_column = config[\"data\"][\"primary_key_column\"]\n",
    "min_date = config[\"data\"][\"min_date\"]\n",
    "events_column_name = config[\"data\"][\"events_column_name\"]\n",
    "events_type_category_column_name = config[\"data\"][\"events_type_category_column_name\"]\n",
    "call_conversion_time_column_name = config[\"data\"][\"call_conversion_time_column_name\"]\n",
    "filter_columns = config[\"data\"][\"filter_columns\"]\n",
    "timestamp_column_name = config[\"data\"][\"timestamp_column_name\"]\n",
    "min_event_interval_in_sec = config[\"analysis\"][\"min_event_interval_in_sec\"]\n",
    "\n",
    "events_type_mapping = reduce(lambda x, y: {**x,**y}, [{val:key for val in list_vals} for key, list_vals in group_events_mapping.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2cebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the output files get stored in the output_directory. Each run of the feature_processing generates a new sub directory based on the timestamp.\n",
    "# output directory structure\n",
    "# - data\n",
    "#   - <run_id>\n",
    "#      - visuals\n",
    "\n",
    "output_directory = os.path.join(local_output_path, run_id)\n",
    "visuals_dir = os.path.join( output_directory, \"visuals\" )\n",
    "\n",
    "logging.info(f\"All the output files will be saved to following location: {output_directory}\")\n",
    "for output_path in [output_directory, visuals_dir]:\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ff68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"WH multi touch data config: database: {database}, schema: {schema}, table: {table}\")\n",
    "\n",
    "table_name =  f\"{database}.{schema}.{table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4049ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Getting data from wh with params:\\n\\tdata: all\\n\\tignoring touches: {ignore_events}\")\n",
    "logging.info(f\"Getting data from wh with params:\\n\\tdata: all\\n\\tignoring touches: {ignore_events}\")\n",
    "input_data = get_raw_data(\n",
    "    table_name = table_name, \n",
    "    snowflake_creds = creds[\"snowflake\"],\n",
    "    min_date = min_date,\n",
    "    timestamp_column=timestamp_column_name,\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ee7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d99ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_conversion_dates = input_data.query(f\"~{call_conversion_time_column_name}.isnull()\").set_index(primary_key_column).to_dict()[call_conversion_time_column_name]\n",
    "raw_data = input_data.drop(columns=['row_id', call_conversion_time_column_name]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations on the raw data. We apply the constraints defined in the constants cell above.\n",
    "\n",
    "def dedup_by_ts_delta(df: pd.DataFrame, primary_key: str, timestamp: str, event_type: str, max_lag: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ### Parameters\n",
    "    1. df : pd.DataFrame\n",
    "        - User touches dataframe. \n",
    "    2. primary_key : str\n",
    "        - column name of the column that contains user_id. \n",
    "    3. timestamp: str\n",
    "        - column name of the column that contains event timestamp\n",
    "    4. event_type: str\n",
    "        - column name of the column that contains event/touch data\n",
    "    5. max_lag: int\n",
    "        - max time (in sec) between consecutive events to be considered as duplicates. \n",
    "\n",
    "    ### Returns\n",
    "    - DataFrame after doing following steps\n",
    "    Based on primary key and event_type, it checks if two consecutive events occur within the max_lag time window. If so, they are considered same event and the latter event is dropped. \n",
    "    \"\"\"\n",
    "    if max_lag <= 0:\n",
    "        return df\n",
    "    df = df.sort_values(by=[primary_key, timestamp], ascending=True).reset_index(drop=True)\n",
    "    original_columns = df.columns\n",
    "    df[\"prev_user\"] = df[primary_key].shift()\n",
    "    df[\"prev_event\"] = df[event_type].shift()\n",
    "    df[\"prev_ts\"] = df[timestamp].shift()\n",
    "\n",
    "    def is_duplicate(row):\n",
    "        if pd.isnull(row[\"prev_user\"]) or pd.isnull(row[\"prev_event\"]) or pd.isnull(row[\"prev_ts\"]):\n",
    "            return False\n",
    "        elif row[primary_key] == row[\"prev_user\"] and row[event_type] == row[\"prev_event\"] and (\n",
    "                row[timestamp] - row[\"prev_ts\"]).total_seconds() <= max_lag:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    df[\"is_duplicate\"] = df.progress_apply(is_duplicate, axis=1)\n",
    "    return df.query(\"is_duplicate==False\")[original_columns].reset_index(drop=True)\n",
    "\n",
    "def process_raw_data(raw_data_df: pd.DataFrame,\n",
    "                     dedup_min_time: int,\n",
    "                     reduce_touches: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ### Parameters\n",
    "    1. raw_data_df : Raw data \n",
    "    2. ignore_touches: Ignores the touches present in this list. \n",
    "    3. min_date: Ignores events before this date\n",
    "    4. dedup_min_time: Time (in sec) between two events of same type. Events that repeat within this interval are combined as one (earlier timestamp is considered)\n",
    "    5. reduce_touches : Whether to combine touchpoints based on their logical groupings\n",
    "\n",
    "    ### Returns\n",
    "    - DataFrame after doing following steps\n",
    "    1. Groups tracks pages if reduce_touches flag is True\n",
    "    2. Deduplicates based on 5 min interval \n",
    "    3. Ignores touches based on ignore_touches list\n",
    "    4. Ðrops events before the min_date timestamp.\n",
    "    \"\"\"\n",
    "    if reduce_touches:\n",
    "        raw_data_df[events_column_name] = raw_data_df[events_column_name].apply(lambda touch: events_type_mapping.get(touch, touch))\n",
    "    \n",
    "    dedup_data_df = (dedup_by_ts_delta(raw_data_df\n",
    "                                       .query(f\"~{events_column_name}.isnull()\",engine='python')\n",
    "                                       .drop_duplicates(),\n",
    "                                       primary_key_column,\n",
    "                                       timestamp_column_name, \n",
    "                                       events_column_name, \n",
    "                                       dedup_min_time)\n",
    "                     .filter(filter_columns)\n",
    "                    )\n",
    "    return dedup_data_df.query(f'{events_column_name} not in @ignore_events', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b78430",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "touch_data_filtered = process_raw_data(raw_data, min_event_interval_in_sec, group_events)\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a301744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the label data to events data\n",
    "touch_data_filtered['is_converted'] = touch_data_filtered[primary_key_column].apply(lambda domain: 1 if domain in domain_conversion_dates else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data:\n",
    "touch_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df34278",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_touchpoints = touch_data_filtered.query(\"is_converted==1\", engine='python').rename(columns={events_column_name: \"event_type\"})\n",
    "negative_touchpoints = touch_data_filtered.query(\"is_converted==0\", engine='python').rename(columns={events_column_name: \"event_type\"})\n",
    "\n",
    "print(\"Summary stats on converted and non converted journeys:\\n\")\n",
    "print(f\"Total rows in converted journeys: {len(positive_touchpoints)}\")\n",
    "print(f\"Distinct converted journeys: {len(positive_touchpoints[primary_key_column].unique())}\")\n",
    "print(f\"Total rows in non-converted journeys: {len(negative_touchpoints)}\")\n",
    "print(f\"Distinct non-converted journeys: {len(negative_touchpoints[primary_key_column].unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a0610",
   "metadata": {},
   "source": [
    "### Part III: Data distribution (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e7465",
   "metadata": {},
   "source": [
    "A few stats on the converted journeys before going into the actual attribution problem:\n",
    "\n",
    "Note: This section is not required to calculate the attribution values. Instead, it is just to show the distribution of no:of events and no:of days before users convert. You can skip directly to [Part IV](#MTA-Calculations-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_summary = positive_touchpoints.query(\"event_type!='webapp'\").groupby([primary_key_column]).agg({timestamp_column_name: \"min\", \"event_type\":[\"size\", \"nunique\"]}).reset_index()\n",
    "conversion_summary.columns = [primary_key_column, timestamp_column_name, \"n_events\", \"n_distinct_events\"]\n",
    "conversion_summary[\"days_to_convert\"] = conversion_summary.apply(lambda row: (domain_conversion_dates.get(row[primary_key_column]) - row[timestamp_column_name]).days, axis=1)\n",
    "\n",
    "conversion_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(16,6))\n",
    "sns.histplot(conversion_summary[\"n_events\"], bins=50, ax=axs[0])\n",
    "axs[0].set_title(\"No:of events before conversion\")\n",
    "axs[0].set_ylabel(\"Conversions\")\n",
    "axs[0].set_xlabel(\"Event count\");\n",
    "\n",
    "sns.histplot(conversion_summary[\"days_to_convert\"], bins=70, ax=axs[1])\n",
    "axs[1].set_title(\"Days to conversion\")\n",
    "axs[1].set_ylabel(\"Conversions\")\n",
    "axs[1].set_xlim([0,100])\n",
    "axs[1].set_xlabel(\"Days since first seen\");\n",
    "\n",
    "plt.savefig(os.path.join(visuals_dir, f\"data_distribution.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978dcfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks percentile counts at various levels. Can modify this list to get a different percentile value (ex: For 90th percentile, add 90 to the list.)\n",
    "percentile_points = [0,5, 25, 50, 75, 95, 99, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0da494",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Avg no:of touches before a user converts: {conversion_summary['n_events'].mean():.2f}; Median: {np.median(conversion_summary['n_events'])}\")\n",
    "print(\"\\nPercentiles for No:of touches:\")\n",
    "(pd.DataFrame.from_dict(\n",
    "    dict(zip(percentile_points, \n",
    "             np.percentile(conversion_summary['n_events'],\n",
    "                           percentile_points).round())),\n",
    "    orient='index', \n",
    "    columns=['n_events'])\n",
    " .reset_index().rename(columns={\"index\":\"percentile\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c222df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Avg days to convert: {conversion_summary['days_to_convert'].mean():.2f}; Median: {np.median(conversion_summary['days_to_convert'])}\")\n",
    "print(\"\\nPercentiles for No:of days to convert:\")\n",
    "(pd.DataFrame.from_dict(\n",
    "    dict(zip(percentile_points, \n",
    "             np.percentile(conversion_summary['days_to_convert'],\n",
    "                           percentile_points).round())),\n",
    "    orient='index', \n",
    "    columns=['n_days_to_convert'])\n",
    " .reset_index().rename(columns={\"index\": \"percentile\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349efe4",
   "metadata": {},
   "source": [
    "<a id='MTA-Calculations-begin'></a>\n",
    "\n",
    "### Part IV: Shapley values calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e439be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function to transform data to the required input form \n",
    "\n",
    "def collect_touchpoints(touchpoints_df: pd.DataFrame, \n",
    "                        primary_key: str=primary_key_column, \n",
    "                        ts_column: str=timestamp_column_name,\n",
    "                        touchpoint_column: str=\"event_type\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform dataframe with each touch as a row to a new dataframe where each journey has a single row with all touches as a list in chronological order\n",
    "    touchpoints_df: A dataframe with each row corresponding to a touch. \n",
    "    primary_key: Name of the column containing unique user identifier\n",
    "    ts_column: Name of column containing timestamp using which journeys are sorted chronologically\n",
    "    touchpoint_column: Name of column containing touch points. \n",
    "    \n",
    "    Returns:\n",
    "    A dataframe with two columns, first has primary key and second has the touchpoint_column\n",
    "    \"\"\"\n",
    "    return (touchpoints_df\n",
    "            .sort_values(by=[primary_key, ts_column], \n",
    "                         ascending=True)\n",
    "            .reset_index(drop=True)\n",
    "            .groupby(primary_key)[touchpoint_column]\n",
    "            .apply(list)\n",
    "            .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "touchpoints_list_pos = collect_touchpoints(positive_touchpoints)\n",
    "\n",
    "touchpoints_list_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Computing normalizing factor for marginal contribution\n",
    "shapley_weight = lambda p, s: (factorial(s)*factorial(p-s-1)/factorial(p))\n",
    "\n",
    "# Getting shapley value of a given channel and values mapping for channel subsets.\n",
    "def compute_shapley_values(v_values_map: dict, \n",
    "                            channel: str,\n",
    "                            n_channels: int) -> float:\n",
    "    # Initiating shap with marginal contribution from S = null subset\n",
    "    shap = 1/n_channels * v_values_map.get(channel, 0) \n",
    "    for subset_str, subset_contrib in v_values_map.items():\n",
    "        subset = subset_str.split(\",\")\n",
    "        if channel not in subset:\n",
    "            subset_union_channel = ','.join(sorted(subset + [channel]))\n",
    "            marginal_contrib = (v_values_map.get(subset_union_channel,0) -\n",
    "                                v_values_map.get(subset_str,0))\n",
    "            shap += shapley_weight(n_channels, len(subset)) * marginal_contrib\n",
    "    \n",
    "    return shap\n",
    "\n",
    "# Helper function to generate subsets for a given set of channels\n",
    "def generate_subsets(touchpoints: List[str]) -> List[List[str]]:\n",
    "    subset_list = []\n",
    "    for subset_size in range(len(touchpoints)):\n",
    "        for subset in itertools.combinations(touchpoints, subset_size + 1):\n",
    "            subset_list.append(list(sorted(subset)))\n",
    "    return subset_list\n",
    "\n",
    "# Computes the utility function value v(s) for given subset S, using contribution values for all subsets.\n",
    "def utility_function(touchpoint_set: List[str], \n",
    "                     contributions_mapping: dict) -> Union[int, float]:\n",
    "    subset_list = generate_subsets(touchpoint_set)\n",
    "    return sum([contributions_mapping.get(','.join(subset),0) for subset in subset_list])\n",
    "\n",
    "\n",
    "# Master function combining all the above functions to compute shapley values for each touchpoint from a list of journeys. \n",
    "def get_shapley_values(journeys_list: List[List[str]], \n",
    "                       contribs_list: List[Union[int, float]])->Dict[str, float]:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        journeys_list (List[List[str]]): List of journeys.\n",
    "         Each journey is a list of touchpoints..\n",
    "        contribs_list (List[Union[int, float]]): List of contributions corresponding to each journey in journeys_list.\n",
    "         Should have same length as journeys_list\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary with key as channel/touchpoint, and Shapley value as its value \n",
    "    \"\"\"\n",
    "    flattened_journeys = [channel for journey in journeys_list for channel in set(journey)]\n",
    "    unique_channels = sorted(list(set(flattened_journeys)))\n",
    "    all_subsets = generate_subsets(unique_channels)\n",
    "    contrib_map = {}\n",
    "    for n, journey in enumerate(journeys_list):\n",
    "        journey_ = \",\".join(sorted(set(journey))) # Ensures deduplication and sorting of journeys\n",
    "        contrib_map[journey_] = contrib_map.get(journey_,0) + contribs_list[n]\n",
    "    v_values = {}\n",
    "    for subset in all_subsets:\n",
    "        v_values[\",\".join(subset)] = utility_function(subset, contrib_map)\n",
    "        \n",
    "    shapley_values = {}\n",
    "    for channel in unique_channels:\n",
    "        shapley_values[channel] = compute_shapley_values(v_values, \n",
    "                                                          channel, \n",
    "                                                          len(unique_channels))\n",
    "    return shapley_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "touches_shapley_values = get_shapley_values(touchpoints_list_pos['event_type'].values, [1] * len(touchpoints_list_pos))\n",
    "touches_shapley_values = pd.DataFrame.from_dict(touches_shapley_values, orient=\"index\").reset_index()\n",
    "touches_shapley_values.columns = [\"touch\", \"shap\"]\n",
    "touches_shapley_values = touches_shapley_values.sort_values(by= 'shap', ascending=False).reset_index(drop=True)\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.barplot(x=\"shap\", y=\"touch\", data=touches_shapley_values, color=\"salmon\")\n",
    "plt.title(\"Shapley Values with converted only journeys\");\n",
    "\n",
    "plt.savefig(os.path.join(visuals_dir, f\"shapley_values_for_converted.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c599ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "touches_shapley_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab20aa3",
   "metadata": {},
   "source": [
    "**Shapley values including negative paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd1e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aggregate_by_journeys(touchpoints_df, identifier_col, touches_col):\n",
    "    \"\"\"\n",
    "    Returns a dataframe where each row contains a sorted list of touchpoints and count of how many times this journey is observed.\n",
    "    Within a journey, order of touches is ignored and a touch is counted only once.\n",
    "    \"\"\"\n",
    "    return (touchpoints_df\n",
    "            .groupby(identifier_col)[touches_col]\n",
    "            .apply(list)\n",
    "            .apply(lambda x: \",\".join(sorted(list(set(x)))))\n",
    "            .reset_index()\n",
    "            .groupby(touches_col)\n",
    "            .size()\n",
    "            .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e120ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_counts = aggregate_by_journeys(positive_touchpoints, primary_key_column, \"event_type\")\n",
    "neg_data_counts = aggregate_by_journeys(negative_touchpoints, primary_key_column, \"event_type\")\n",
    "pos_data_counts.columns = ['events', 'conversions']\n",
    "neg_data_counts.columns = ['events', 'non_conversions']\n",
    "\n",
    "all_data_counts = pos_data_counts.merge(neg_data_counts, on=\"events\", how=\"outer\").fillna(0)\n",
    "all_data_counts[\"conversion_rate\"] = all_data_counts.apply(lambda row: row[\"conversions\"]/(row[\"conversions\"] + row[\"non_conversions\"]), axis=1)\n",
    "all_data_counts['events'] = all_data_counts['events'].apply(lambda x: x.split(\",\"))\n",
    "del pos_data_counts, neg_data_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "touches_shapley_values_all_paths = get_shapley_values(all_data_counts['events'].values, all_data_counts['conversion_rate'])\n",
    "\n",
    "touches_shapley_values_all_paths = pd.DataFrame.from_dict(touches_shapley_values_all_paths, orient=\"index\").reset_index()\n",
    "touches_shapley_values_all_paths.columns = [\"touch\", \"shap\"]\n",
    "touches_shapley_values_all_paths = touches_shapley_values_all_paths.sort_values(by= 'shap', ascending=False).reset_index(drop=True)\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.barplot(x=\"shap\", y=\"touch\", data=touches_shapley_values_all_paths, color=\"salmon\")\n",
    "plt.title(\"Shapley Values including non converted users\");\n",
    "\n",
    "plt.savefig(os.path.join(visuals_dir, f\"shapley_values_non_converted_included.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging both forms of shapley values\n",
    "touches_shapley_values_all_paths['shap'] = touches_shapley_values_all_paths['shap']/touches_shapley_values_all_paths['shap'].sum()\n",
    "touches_shapley_values['shap_normalized'] = touches_shapley_values['shap']/touches_shapley_values['shap'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a506fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_vals = (touches_shapley_values_all_paths\n",
    "            .rename(columns={\"shap\":\"shap_all_paths\"})\n",
    "            .merge(touches_shapley_values\n",
    "                   .filter([\"touch\", \"shap_normalized\"])\n",
    "                   .rename(columns={'shap_normalized':'shap_conversions'}),\n",
    "                   on='touch'))\n",
    "mta_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df818e",
   "metadata": {},
   "source": [
    "### Part V: Markov Chain Values Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b2593",
   "metadata": {},
   "source": [
    "Index of transition counts: 1st: source, last: destination. 2 to -1: same order as dict_touches_inv keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "touchpoints_list_neg = collect_touchpoints(negative_touchpoints)\n",
    "\n",
    "all_touches = list(positive_touchpoints[\"event_type\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c26d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_transition_counts(journey_list: List[List[str]], \n",
    "                               distinct_touches_list: List[str], \n",
    "                               is_positive: bool):\n",
    "    if is_positive:\n",
    "        destination_idx = -1\n",
    "    else:\n",
    "        destination_idx = -2\n",
    "    transition_counts = np.zeros(((len(distinct_touches_list)+3), (len(distinct_touches_list)+3)))\n",
    "    for journey in journey_list:\n",
    "        transition_counts[0, (distinct_touches_list.index(journey[0])+1)] += 1 # First point in the path\n",
    "        for n, touch_point in enumerate(journey):\n",
    "            if n == len(journey) - 1:\n",
    "                # Reached last point\n",
    "                transition_counts[(distinct_touches_list.index(touch_point)+1), destination_idx] += 1\n",
    "                transition_counts[destination_idx, destination_idx]+=1\n",
    "            else:\n",
    "                transition_counts[(distinct_touches_list.index(touch_point)+1), (distinct_touches_list.index(journey[n+1]) + 1)] +=1\n",
    "    transition_labels = distinct_touches_list.copy()\n",
    "    transition_labels.insert(0, \"Start\")\n",
    "    transition_labels.extend([\"Dropoff\", \"Converted\"])\n",
    "    return transition_counts, transition_labels\n",
    "\n",
    "row_normalize_np_array = lambda transition_counts: transition_counts / transition_counts.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "def plot_transitions(transition_probabilities: np.array, labels: List[str], title=\"Transition Probabilities\", show_annotations=True):\n",
    "    ax = sns.heatmap(transition_probabilities,\n",
    "                     linewidths=0.5,\n",
    "                     robust=True, \n",
    "                     annot_kws={\"size\":8}, \n",
    "                     annot=show_annotations,\n",
    "                     fmt=\".2f\",\n",
    "                     cmap=\"YlGnBu\",\n",
    "                     xticklabels=labels,\n",
    "                     yticklabels=labels)\n",
    "    ax.tick_params(labelsize=10)\n",
    "    ax.figure.set_size_inches((16, 10))\n",
    "    ax.set_ylabel(\"Previous Step\")\n",
    "    ax.set_xlabel(\"Next Step\")\n",
    "    ax.set_title(title);\n",
    "\n",
    "\n",
    "def get_transition_probabilities(converted_touchpoints_list: List[List[int]], \n",
    "                                 dropoff_touchpoints_list: List[List[int]], \n",
    "                                 distinct_touches_list: List[str], \n",
    "                                 visualize=False) -> Tuple[np.array, List[str]]:\n",
    "    pos_transitions, _ = generate_transition_counts(converted_touchpoints_list, distinct_touches_list, is_positive=True)\n",
    "    neg_transitions, labels = generate_transition_counts(dropoff_touchpoints_list, distinct_touches_list, is_positive=False)\n",
    "    all_transitions = pos_transitions + neg_transitions\n",
    "    transition_probabilities = row_normalize_np_array(all_transitions)\n",
    "    if visualize:\n",
    "        plot_transitions(transition_probabilities, labels, show_annotations=True)\n",
    "    return transition_probabilities, labels\n",
    "\n",
    "def converge(transition_matrix, max_iters=200, verbose=True):\n",
    "    T_upd = transition_matrix\n",
    "    prev_T = transition_matrix\n",
    "    for i in range(max_iters):\n",
    "        T_upd = np.matmul(transition_matrix, prev_T)\n",
    "        if np.abs(T_upd - prev_T).max()<1e-5:\n",
    "            if verbose:\n",
    "                print(f\"{i} iters taken for convergence\")\n",
    "            return T_upd\n",
    "        prev_T = T_upd\n",
    "    if verbose:\n",
    "        print(f\"Max iters of {max_iters} reached before convergence. Exiting\")\n",
    "    return T_upd\n",
    "\n",
    "\n",
    "def get_removal_affects(transition_probs, labels, ignore_labels=[\"Start\", \"Dropoff\",\"Converted\"], default_conversion=1.):\n",
    "    removal_affect = {}\n",
    "    for n, label in enumerate(labels):\n",
    "        if label in ignore_labels:\n",
    "            continue\n",
    "        else:\n",
    "            drop_transition = transition_probs.copy()\n",
    "            drop_transition[n,:] = 0.\n",
    "            drop_transition[n,-2] = 1.\n",
    "            drop_transition_converged = converge(drop_transition, 500, False)\n",
    "            removal_affect[label] = default_conversion - drop_transition_converged[0,-1]\n",
    "    return removal_affect\n",
    "\n",
    "def get_markov_attribution(tp_list_positive: List[List[int]],\n",
    "                           tp_list_negative: List[List[int]], \n",
    "                           distinct_touches_list: List[str], \n",
    "                           visualize=False) -> Tuple[Dict[str, float], np.array]:\n",
    "    transition_probabilities, labels = get_transition_probabilities(tp_list_positive, tp_list_negative, distinct_touches_list, visualize=visualize)\n",
    "    transition_probabilities_converged = converge(transition_probabilities, max_iters=500, verbose=False)\n",
    "    removal_affects = get_removal_affects(transition_probabilities, labels, default_conversion=transition_probabilities_converged[0,-1])\n",
    "    total_conversions = len(tp_list_positive)\n",
    "    attributable_conversions = {}\n",
    "    total_weight = sum(removal_affects.values())\n",
    "    for tp, weight in removal_affects.items():\n",
    "        attributable_conversions[tp] = weight/total_weight * total_conversions\n",
    "    return attributable_conversions, transition_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b083a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_attribution_values, transition_probabilities = get_markov_attribution(touchpoints_list_pos[\"event_type\"].values, touchpoints_list_neg[\"event_type\"].values, all_touches, visualize=True)\n",
    "plt.savefig(os.path.join(visuals_dir, f\"markov_transition_probabilities.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc6a57",
   "metadata": {},
   "source": [
    "In the above graphic, we can see the transition probabilities from each touch (Y-axis) to the next touch (X-axis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95234408",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_transitions, labels = generate_transition_counts(touchpoints_list_pos[\"event_type\"].values, all_touches, is_positive=True)\n",
    "neg_transitions, labels = generate_transition_counts(touchpoints_list_neg[\"event_type\"].values, all_touches, is_positive=False)\n",
    "all_transitions = pos_transitions + neg_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b369296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs=plt.subplots(1,2, figsize=(18, 5))\n",
    "sns.set_style(\"white\")\n",
    "sns.barplot(x=labels[:-2], y=100*all_transitions[:-2, -2]/all_transitions[:-2, -2].sum(), ci=None, color=\"salmon\", ax=axs[0])\n",
    "    \n",
    "axs[0].set_xticklabels(labels[:-2],rotation=60)\n",
    "axs[0].set_ylabel(\"Percent\")\n",
    "axs[0].set_title(\"Distribution of last touch before dropoff\");\n",
    "\n",
    "sns.barplot(x=labels[:-2], y=100*all_transitions[:-2, -1]/all_transitions[:-2, -1].sum(), ci=None, color=\"salmon\", ax=axs[1])\n",
    "axs[1].set_xticklabels(labels[:-2],rotation=60)\n",
    "axs[1].set_ylabel(\"Percent\")\n",
    "axs[1].set_title(\"Distribution of last touch before convert\");\n",
    "\n",
    "plt.savefig(os.path.join(visuals_dir, f\"distribution_of_last_touch_before_dropoff_and_convert.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee27fd6",
   "metadata": {},
   "source": [
    "### Part VI: Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_touch_based_mta = touchpoints_list_pos[\"event_type\"].apply(lambda event_list: event_list[-1]).value_counts().reset_index()\n",
    "last_touch_based_mta.columns = ['touch', 'last_touches']\n",
    "last_touch_based_mta['last_touches'] = last_touch_based_mta['last_touches']/last_touch_based_mta['last_touches'].sum()\n",
    "\n",
    "markov_attr_values_df = pd.DataFrame.from_dict(markov_attribution_values, orient=\"index\").reset_index()\n",
    "markov_attr_values_df.columns = [\"touch\", \"markov\"]\n",
    "markov_attr_values_df['markov'] = markov_attr_values_df['markov']/markov_attr_values_df['markov'].sum()\n",
    "mta_values = markov_attr_values_df.merge(mta_vals, on=\"touch\", how=\"outer\").merge(last_touch_based_mta, on='touch',how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_long = pd.melt(mta_values, \"touch\", [\"markov\",  \"shap_conversions\", \"last_touches\"])\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.barplot(data=mta_long, x='touch',y='value',hue='variable');\n",
    "plt.xticks(rotation=90);\n",
    "plt.savefig(os.path.join(visuals_dir, f\"results_summary.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_values.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc0c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "SnowflakeDataIO.write_to_snowflake_table(mta_values, creds[\"snowflake\"][\"results_table_name\"], creds[\"snowflake\"], if_exists=\"append\")\n",
    "print(f'The output data is stored in the warehouse table: {creds[\"snowflake\"][\"results_table_name\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa833db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MTA values will be written to the location:\\n\\t{output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a140392",
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_values.to_parquet(f\"{output_directory}/mta_values.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2480784c",
   "metadata": {},
   "source": [
    "### Part VII: Robustness testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30783a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_vals = {}\n",
    "shapley_vals = {}\n",
    "\n",
    "touchpoints_list_pos_rand1, touchpoints_list_pos_rand2 = train_test_split(touchpoints_list_pos, train_size=0.5)\n",
    "touchpoints_list_neg_rand1, touchpoints_list_neg_rand2 = train_test_split(touchpoints_list_neg, train_size=0.5)\n",
    "\n",
    "touches_shapley_values_rand1 = get_shapley_values(touchpoints_list_pos_rand1['event_type'].values, [1] * len(touchpoints_list_pos_rand1))\n",
    "# get_shapley_values(touchpoints_list_pos_rand1[\"event_id\"].values, dict_touches, visualize=False)\n",
    "markov_attribution_values_rand1, _ = get_markov_attribution(touchpoints_list_pos_rand1[\"event_type\"].values, touchpoints_list_neg_rand1[\"event_type\"].values, all_touches, visualize=False)\n",
    "\n",
    "touches_shapley_values_rand2 = get_shapley_values(touchpoints_list_pos_rand2['event_type'].values, [1] * len(touchpoints_list_pos_rand2))\n",
    "\n",
    "markov_attribution_values_rand2, _ = get_markov_attribution(touchpoints_list_pos_rand2[\"event_type\"].values, touchpoints_list_neg_rand2[\"event_type\"].values, all_touches, visualize=False)\n",
    "\n",
    "\n",
    "for key, val in touches_shapley_values_rand1.items():\n",
    "    shapley_vals[key] = [val, touches_shapley_values_rand2.get(key,0)]\n",
    "\n",
    "for key, val in markov_attribution_values_rand1.items():\n",
    "    markov_vals[key] = [val, markov_attribution_values_rand2.get(key,0)]\n",
    "\n",
    "    \n",
    "shapley_vals_df = pd.DataFrame.from_dict(shapley_vals, orient='index')\n",
    "markov_vals_df = pd.DataFrame.from_dict(markov_vals, orient='index')\n",
    "print(f\"Correlation of shapley values between two non-overlapping splits: {shapley_vals_df.corr()[0][1]:.3f}\")\n",
    "print(f\"Correlation of markov values between two non-overlapping splits: {markov_vals_df.corr()[0][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapley_vals_df = pd.melt(shapley_vals_df.reset_index(), id_vars='index')\n",
    "shapley_vals_df.columns = ['touch', 'iter', 'shap']\n",
    "\n",
    "markov_vals_df = pd.melt(markov_vals_df.reset_index(), id_vars='index')\n",
    "markov_vals_df.columns = ['touch', 'iter', 'markov']\n",
    "\n",
    "tp_order = shapley_vals_df.groupby('touch')['shap'].mean().reset_index().sort_values(\"shap\", ascending=False)['touch'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c30a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16,6))\n",
    "sns.barplot(x='touch', y='shap', hue='iter', data=shapley_vals_df, ax=axs[0], order=tp_order)\n",
    "for item in axs[0].get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "axs[0].set_title(\"Shapley vals for touchpoints in non-overlapping splits\");\n",
    "\n",
    "sns.barplot(x='touch', y='markov', hue='iter', data=markov_vals_df, ax=axs[1], order=tp_order)\n",
    "for item in axs[1].get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "axs[1].set_title(\"Markov vals for touchpoints in non-overlapping splits\");\n",
    "\n",
    "plt.savefig(os.path.join(visuals_dir, f\"shapley_markov_values_non_overlapping_splits.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_vals = {}\n",
    "shapley_vals = {}\n",
    "for iters in range(10):\n",
    "    touchpoints_list_pos_rand, _ = train_test_split(touchpoints_list_pos, train_size=0.7)\n",
    "    touchpoints_list_neg_rand, _ = train_test_split(touchpoints_list_neg, train_size=0.7)\n",
    "    touches_shapley_values_rand = get_shapley_values(touchpoints_list_pos_rand[\"event_type\"].values, [1] * len(touchpoints_list_pos_rand))\n",
    "    markov_attribution_values_rand, _ = get_markov_attribution(touchpoints_list_pos_rand[\"event_type\"].values, touchpoints_list_neg_rand[\"event_type\"].values, all_touches, visualize=False)\n",
    "    for touch, shap in touches_shapley_values_rand.items():\n",
    "        curr = shapley_vals.get(touch, [])\n",
    "        curr.append(shap)\n",
    "        shapley_vals[touch] = curr\n",
    "    for touch, mark in markov_attribution_values_rand.items():\n",
    "        curr = markov_vals.get(touch, [])\n",
    "        curr.append(mark)\n",
    "        markov_vals[touch] = curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef00dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapley_ranks = pd.DataFrame.from_dict(shapley_vals, orient='index').fillna(0).rank(axis=0, ascending=False).astype(int)\n",
    "markov_ranks = pd.DataFrame.from_dict(markov_vals, orient='index').fillna(0).rank(axis=0, ascending=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapley_ranks.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae065ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_ranks.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f51f3f",
   "metadata": {},
   "source": [
    "Shapley values and markov values rank order remain quite stable with only minor differences, as can be seen above\n",
    "\n",
    "Markov is significantly more robust compared to Shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b7e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ea2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell to hide code while converting to a html page\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "$('div.input').hide();\n",
    "</script>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f019c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb9dda7c9d815e1cdc337d7fe8d5832923daae4c84b0a4e15a32dd1f30943ba5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rlabs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
